
# 引例

用决策树把一个平面上的众多点分为两类，每一个点都有（x1，x2）两个特征，下面展示分类的过程，
![图片.png](https://upload-images.jianshu.io/upload_images/1936544-2dd62f835ecc96c5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


最后生成的决策树，取了四个分割点，在图上的显示如下，只要是落在中央矩形区域内默认是绿色，否则为红色，

![图片.png](https://upload-images.jianshu.io/upload_images/1936544-433b420bd29a4f68.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

不过这种情况是分类参数选择比较合理的情况（它不介意某些绿色的点落在外围），但是当在训练的时候需要将所有的绿点无差错的分出来（即参数选择不是很合理的情况），决策树会产生过拟合的现象，导致泛化能力变弱。

# 升华

单纯的决策树是一种非黑即白的分类，这种方式从数学上或者哲学上去理解，相当于工作在低维，缺乏高维抽象能力。用生活中的话说，就是”不够圆滑“。因此引入随机森林这些，相当于提升到了高维一样。
> 参考上面两幅图的对比。综合准确率和泛化性能来说，第一个图的圆圈分类比较好；但是决策树”直来直去“。当然极限情况下，足够多足够短的直线确实可以拟合圆，但是想想这复杂度。。。

再结合 SVM，核函数可以升维。所以随机森林之于决策树，可以理解为核函数之于 SVM。他们都有升高维度的作用。

有句名言叫做：软件里面的所有问题都可以通过加一个中间层来解决。
换到数学中来类比：数学里面的某些问题（学识有限，不敢太绝对地说所有问题）都可以通过升高维度来解决。

升维度其实在生活中很常见。只说美女，大家脑海里浮现的面孔实在是太多了。如果加一个条件（数学上可以叫做升高一个维度），比如：大眼睛、身高 170+、体重 100 斤以下、已婚、等等。这样范围就变小了，相当于提高了精度一样。
> 例子不太恰当，能意会意思就好。
