
先要明确前提：较少变更的数据才适合做缓存

接着看看引入缓存会带来的问题：

*   缓存的引入带来了`一致性`问题，需要处理缓存中的数据与原数据不一致的问题；

*   缓存的引入增加了软件架构的`复杂性`；

*   缓存过期是个难题，这个问题主要体现在`何时更新缓存`上；

# 一、缓存更新策略

## 1.1 被动失效

设置 key 过期的时间，让其自动失效

缺点：被动失效策略中存在一个问题，就是从缓存失效开始直到新的数据再次被更新到缓存中的这段时间，所有的读请求都将会直接落到 DB 上。对于一个大访问量的系统来说，这有可能会带来风险。

所以有时候需要换一种策略：当数据库更新时，主动去更新缓存，这样在缓存数据的整个生命期内，就不会有空窗期。

## 1.2 主动更新

### 1.2.1 先更新谁

**先更新缓存，后更新 DB**

假设有如下的执行系列：

（1）t1 更新缓存；
（2）t2 读缓存，因为 t1 把缓存更新了，导致 t2 没读到。从 DB 中读，然后更新缓存；
（3）t1 更新 DB。

> 上述操作系列会导致缓存脏数据。

**先更新 DB，后更新缓存**

假设有如下操作序列：

（1）t1 更新 DB；
（2）t2 更新 DB；
（3）t2 更新缓存；
（4）t1 更新缓存。

> 上述操作系列同样会导致缓存脏数据。

**结论：**无论谁先谁后，只要`更新缓存和更新 DB 不是原子的，就可能导致不一致`。但是从实际业务来讲，一般缓存也都是保持“最终一致性”，而不是和 DB 的“强一致性”。

实际中，一般建议先更新 DB，再更新缓存，优先保证 DB 数据正确。

但如果一定要“强一致性”，就不能用上面的解决方案了。

### 1.2.2 缓存更新策略

**写时失效**

这种策略比较简单，尤其在一些缓存数据需要多步计算才能出来的场景；缺点在于失效后并发读时，会有多个请求同时打到 DB。

这种策略下会有脏数据。比如，一个是读操作，但是没有命中缓存，然后就到数据库中取数据，此时来了一个写操作，写完数据库后，让缓存失效，然后，之前的那个读操作再把老的数据放进去，所以，会造成脏数据。

> 但这个case理论上会出现，实际上出现的概率可能非常低，因为这个条件需要发生在读缓存时缓存失效，而且并发着有一个写操作。而实际上数据库的写操作会比读操作慢得多，而且还要锁表，而读操作必需在写操作前进入数据库操作，而又要晚于写操作更新缓存，所有的这些条件都具备的概率并不大。

**写时更新**

这种策略会提高写数据时的开销，因为写操作不仅要写DB，还要同时更新缓存。不过带来的效果要看场景：如果是读多写少的场景，那么写时延迟的牺牲，换来更高的读效率是值得的；但是如果是写多读少的场景，那么用`写时失效`的策略更好。

这种策略在存在两个并发的写操作时会导致脏数据，[Why does Facebook use delete to remove the key-value pair in Memcached instead of updating the Memcached during write request to the backend?](https://www.quora.com/Why-does-Facebook-use-delete-to-remove-the-key-value-pair-in-Memcached-instead-of-updating-the-Memcached-during-write-request-to-the-backend)

> 就像上面这个链接中说的，要么通过 2PC 或是 Paxos 协议保证一致性，要么就是尽可能地降低并发时脏数据的概率，Facebook 使用了这个降低概率的方法（2PC太慢，Paxos太复杂）。

两种方式的选择也和代码复杂度相关。

一般情况下：

*   简单数据场景：写时更新比较好；

*   复杂数据场景：选择写时失效。

**Write Back**

Linux 内核的页缓存算法。

简单说就是，在更新数据的时候，只更新缓存，不更新数据库；而缓存会异步地批量更新数据库。

这个设计的好处就是让数据的I/O操作很快，因为异步，write back还可以合并对同一个数据的多次操作，所以性能的提高相当可观。

但是，其带来的问题是，数据不是强一致性的，而且可能会丢失（Unix/Linux非正常关机会导致数据丢失就是因为这个）。

> 在软件设计上，我们基本上不可能做出一个没有缺陷的设计，就像算法设计中的时间换空间，空间换时间一个道理，必须做出取舍。

> 另外，Write Back 实现逻辑比较复杂，因为他需要 track 有哪数据是被更新了的，需要刷到持久层上。操作系统的 write back 会在仅当这个 cache 需要失效的时候，才会被真正持久起来，比如，内存不够了，或是进程退出了等情况(lazy write)。

## 1.3 缓存淘汰机制

缓存淘汰机制不要和前面的更新机制混淆。淘汰机制是从缓存自身的角度来说的，缓存更新机制是从缓存和 DB 数据同步的角度来说的。

> 因为缓存只能够有限的使用内存，任何 Cache 系统都需要一个如何淘汰缓存的方案

同操作系统的页置换算法。

# 二、缓存穿透

指某个 key，先查 cache 没查到，再查 DB 也没有查到。

这种 key 的存在，会导致 cache 一直没办法命中，压力一直打在 DB 上面。如果访问很高频，可能会压垮 DB。

解决办法：当查询 DB 没查到时，往缓存中写入一个空值（缺省值），这样以后再查，就不会打到 DB 上了。

# 三、缓存雪崩

一般是由于某个节点失效，导致其它节点的缓存命中率下降，缓存中缺失的数据直接去数据库查询，短时间内造成数据库服务器崩溃。

或者是由于缓存周期性失效，比如设置每隔 6 个小时失效一次，那么每6个小时将会有一个请求峰值，严重的话，也会导致数据库崩溃。

重启 DB 后，短期内又被压垮，但缓存又会恢复一点，DB 反复重启多次，直至缓存重建完毕，才能恢复稳定。

此问题典型的负载图如下，周期性的出现峰值，且周期和缓存失效的周期几乎吻合。

[图片上传中...(image-89d3d5-1603872331251-2)] 

# 四、缓存无底洞

Facebook 的工作人员反应 2010 年已达到 3000 个 memcached 节点，储存数千 G 的缓存。

他们发现一个问题：memcached 的连接效率下降了，于是增加了Memcache节点，添加之后，发现因为连接频率导致的问题，仍然存在看，并没有好转。

原文请见： [Facebook's Memcached Multiget Hole: More Machines != More Capacity](http://highscalability.com/blog/2009/10/26/facebooks-memcached-multiget-hole-more-machines-more-capacit.html)

以会员信息为例：

<colgroup><col><col></colgroup>
| key | value |
| user-133-age | 22 |
| user-133-height | 170 |
| user-89-age | 60 |
| user-89-height | 182 |

当服务器增多，133号用户的信息也被散落在更多的服务器，所以，同样是访问个人主页，得到的相同的个人信息，节点越多，要连接的节点也越多，对于 memcached 的连接数，并没有随着节点的增多而降低，问题出现。

[图片上传中...(image-a30120-1603872331251-1)] [图片上传中...(image-2ae48d-1603872331251-0)] 

解决策略：可以将某一组key，按其共同前缀来分布，比如按照`user-133`来计算，而不是以`user-133-age，user-133-name，user-133-height`来计算，这样 3 个关于个人信息的 key ，都落在同一个节点，访问个人主页时，只需连接一个节点。

# 五、永久数据被踢

问题描述：自己设置的永久数据，莫名其妙的丢失了。

可能原因：LRU淘汰机制

最佳实践：永久数据和非永久数据分开放。

# 六、缓存实现分布式锁

分布式锁一般有三种实现：

*   基于数据库实现分布式锁

*   基于缓存（redis，memcached，tair）实现分布式锁

*   基于Zookeeper实现分布式锁

用缓存实现分布式锁是因为缓存比较轻量，并且缓存的响应快、吞吐高。**最重要的是还有自动失效的机制来保证锁一定能释放**。

**优点**：实现简单，吞吐量高，对于高并发情况应付自如，自带超时保护，对于网络抖动的情况也可以利用超时删除策略保证不会阻塞所有流程。

**缺点**：单点问题、没有线程唤醒机制、网络抖动可能会引起锁删除失败，且通过超时时间来控制锁的失效时间并不是十分的靠谱。

这一块目前只有一个宏观的认识，还需要以后继续学习。

> 利用 Zookeeper 不能重复创建一个节点的特性来实现一个分布式锁，和 redis 实现分布式锁很像。

参考文章：

[分布式锁的几种实现方式~](http://www.hollischuang.com/archives/1716)
[分布式锁的三种实现的对比](http://www.jianshu.com/p/c2b4aa7a12f1)

# 七、其他

「 Q：redis官方可是号称每秒8W次的写入速度，这不就浪费了😂 」

「 A：redis的写入速度很高，读取数据更高，把网卡打到极限了，服务还很正常。 最早的时候，这么设计是因为redis最初提出来的时候并没把自己定位成缓存，而是列存储，这么一来，它的写性能必定不能差，不然没法做一个存储服务来用的。 后来随着数据量的爆发，以及reids 数据同步、集群方案的一些问题，所以后来慢慢被大家当做缓存来用。 」

# 八、感想

原来觉得能做系统架构的人一定是很有经验的，越来越感觉宏观系统架构中的很多设计都来源于操作系统等这些微观的东西。比如，

*   云计算中的很多虚拟化技术的原理，和传统的虚拟内存、进程模型很像

*   Unix 下的那些 I/O 模型，也放大到了架构里的同步异步的模型

*   Unix 发明的管道不就是数据流式计算架构吗

*   ......

仔细看看这些微观层面，会发现有很多设计都非常精妙，也非常巧妙……

**所以，如果要做好架构，首先得把计算机体系结构以及很多老古董的基础技术吃透了。**
